#!/usr/bin/env python3
'''
BLEU (Bilingual Evaluation Understudy) and ROUGE-L F1 are metrics commonly used to evaluate the quality of text
that has been machine-translated from one language to another (BLEU) and to assess the quality of summaries (ROUGE-L).

BLEU:
- BLEU measures the similarity between a candidate translation of text and one or more reference translations.
- It does so by calculating the precision of n-grams (contiguous sequence of n items from a given sample of text or speech)
in the candidate text relative to the reference text(s), then applying a brevity penalty to penalize overly short translations.
- The core idea is to check how many words and phrases in the candidate text appear in the reference text(s),
adjusting for the length of the candidate to avoid favoring overly short or trivial translations.
- Calculation:
    1. For each n-gram (up to a maximum length), calculate the number of n-grams in the candidate translation
    that appear in any reference translation.
    2. Compute the precision for each n-gram level by dividing the number of overlapping n-grams by the
    total number of n-grams in the candidate translation.
    3. Compute a geometric mean of the precisions, applying a brevity penalty if the candidate translation is shorter
    than the reference translations.

ROUGE-L F1:
- ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating automatic summarization
and machine translation. ROUGE-L specifically focuses on the longest common subsequence (LCS) between the candidate text
and reference texts.
- LCS is a sequence that appears in the same relative order in both the candidate and reference texts but not necessarily
in a contiguous block.
- ROUGE-L F1 is an F1 score that considers both the recall and precision of the longest common subsequences, aiming to balance them.
- Calculation:
    1. Identify the longest common subsequence between the candidate summary and each reference summary.
    2. Calculate recall as the length of the LCS divided by the length of the reference summary.
    3. Calculate precision as the length of the LCS divided by the length of the candidate summary.
    4. Calculate the F1 score as the harmonic mean of precision and recall, providing a single measure that balances both.

Both metrics are designed to compare a generated piece of text against one or more reference texts,
offering a quantitative measure of the generated text's quality or relevance.
However, they are not perfect and can sometimes fail to capture semantic accuracy or the fluency of the text.
'''

import evaluate

# Define the metrics and their descriptions
metric_descriptions = {
    'BLEU': '''
    - Measures the similarity between the generated text and one or more reference texts.
    - Primarily used for evaluating the quality of machine-translated text against human translations.
    - Scores range from 0 to 1, with higher scores indicating better similarity to the reference texts.
    - Calculated based on the precision of matched n-grams between the generated and reference texts, adjusted for sentence length.
    ''',
    'ROUGE-L F1': '''
    - Evaluates the longest common subsequence (LCS) between the generated text and reference texts.
    - Suitable for tasks like summarization where capturing the essence of the content is more important than exact wording.
    - Considers both precision and recall, leading to the F1 score which balances the two.
    - Scores range from 0 to 1; higher scores indicate better overlap with the reference texts.
    '''
}


# Define the interpretation of scores
def interpret_score(metric_name, score):
    '''
    Interprets the score of a metric based on its name.

    :param metric_name: Name of the metric (e.g., "BLEU", "ROUGE-L F1").
    :param score: The score of the metric.
    :return: A string describing the interpretation of the score.
    '''
    if metric_name in ["BLEU", "ROUGE-L F1"]:
        if score > 0.5:
            return "Good - High similarity to reference text."
        elif score > 0.25:
            return "Fair - Some similarity to reference text."
        else:
            return "Poor - Low similarity to reference text."
    return "Unknown metric or score."


def compute_metrics(task_name, generated_text, gt):
    """
    Computes evaluation metrics for generated text against a ground truth, tailored to the specific task.

    Args:
        task_name (str): Name of the task (e.g., "Creative Writing", "Summarization").
        generated_text (str): Text generated by the model.
        gt (str): Ground truth text for comparison.

    Returns:
        dict: A dictionary with metric names as keys and their scores as values.
        str: The name of the primary evaluation metric used.
        float: The score of the primary evaluation metric.
    """
    if task_name == "Creative Writing":
        metric = evaluate.load("bleu")
        predictions = [generated_text]  # Predictions should be a list of strings
        references = [[gt]]  # References should be a list of lists of strings for BLEU
        score = metric.compute(predictions=predictions, references=references)
        bleu_score = score.get('bleu', 0)  # Default to 0 if BLEU score is not found
        return {"bleu": bleu_score}, "BLEU", bleu_score

    elif task_name == "Summarization":
        metric = evaluate.load("rouge")
        predictions = [generated_text]  # Predictions should be a list of strings
        references = [[gt]]  # References should be a list of lists of strings for ROUGE
        score = metric.compute(predictions=predictions, references=references)
        # Accessing the F1 score for ROUGE-L directly based on the structure provided
        rouge_l_f1 = score.get('rougeL', 0)  # Default to 0 if ROUGE-L score is not found
        return {"rouge-l": rouge_l_f1}, "ROUGE-L F1", rouge_l_f1

    else:
        return {}, "None", 0.0


def compute_aggregate_metrics(task_name, all_generated_texts, all_ground_truths):
    """
    Computes aggregate metrics for a set of generated texts against their ground truths.

    Args:
        task_name (str): Name of the task (e.g., "Creative Writing", "Summarization").
        all_generated_texts (list of str): All texts generated by the model.
        all_ground_truths (list of str): All ground truth texts for comparison.

    Returns:
        dict: A dictionary with metric names as keys and their aggregate scores as values.
    """
    metrics_scores = {}

    metric = evaluate.load("bleu")
    scores = metric.compute(predictions=all_generated_texts, references=[[gt] for gt in all_ground_truths])
    metrics_scores["BLEU"] = scores["bleu"]

    metric = evaluate.load("rouge")
    scores = metric.compute(predictions=all_generated_texts, references=all_ground_truths)
    metrics_scores["ROUGE-L F1"] = scores.get('rougeL', 0)  # Default to 0 if ROUGE-L score is not found

    return metrics_scores
